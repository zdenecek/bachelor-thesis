\chapter{Analysis}
There are many benefits to having \glsxtrfull{dqm} processes in place. In fact, in today's data-driven environments, the assurance of data quality is not just a preference but a critical necessity. Organizations rely on accurate, timely, and reliable data to make informed decisions, drive strategies, and optimize operations. As such, the field of \glsxtrshort{dqm} has evolved to address these needs through sophisticated tools and methodologies. However, the effective implementation of these tools requires a deep understanding of both the tools themselves and the roles of those who interact with them.


This chapter delves into the analysis of the current landscape of \glsxtrshort{dqm}, focusing particularly on the need for programmatic access to these tools. This need stems from the growing requirement to seamlessly integrate data quality solutions into existing data pipelines, a task that typically falls within the purview of data engineers. As we explore this topic, we will discuss the challenges faced by data engineers and the crucial role of data stewards in ensuring the success of data quality initiatives. The insights gained here will lay the groundwork for understanding how programmatic access can bridge the gap between manual data quality oversight and automated, scalable data governance frameworks.


\section{The need for programmatic access}

In the context of \glsxtrshort{dqm}, most commonly, the center point of the focus is on the persona of the so-called 'data steward.' This is because data stewards are crucial
for any data quality initiative. Data stewards must be involved in setting up data quality processes to ensure success. This includes tasks such as defining data quality rules,
profiling, and addressing data quality issues. [What is a data steward? https://www.ataccama.com/blog/what-is-a-data-steward]

In the world of \glsxtrshort{dqm}, there arises a need to consume data quality tools programmatically. This is where the persona of a data engineer comes into play. There are more reasons for this, in simple terms,
the necessity is a corollary to the need to integrate data quality tools into data pipelines, which is what data engineers are responsible for.

 It follows that any solution aimed at pipeline integration should be designed to accommodate data engineers as they will be its users. In the following sections, we will discuss the needs of data engineers in the context of \glsxtrshort{dqm} tools.

\section{Data Quality Management Tools at Ataccama}

One of the major players in the field of \glsxtrshort{dqm} tools is Ataccama. 

Ataccama ONE is a data management application that provides a wide range of features for \glsxtrshort{dqm}, such as Data cataloging, Master data management, and Data visualizations. Most importantly, Ataccama ONE includes a broad suite of \glsxtrshort{dqm} tools. This includes data profiling, data quality rules, data quality monitoring, and data quality issue resolution. Ataccama ONE also provides a wide range of connectors to various data sources and data targets.

Ataccama is a powerful tool for \glsxtrshort{dqm}, but it has some limitations. One of the limitations of Ataccama ONE is that it is designed for something other than programmatic access. The main interface for Ataccama ONE is a web interface. This means it takes work to integrate Ataccama into data pipelines.


\subsection{Data Quality Rules at Ataccama}

Data quality rules are essential components of any comprehensive data management strategy, and Ataccama ONE excels in providing robust capabilities for defining and enforcing these rules. Ataccama's approach to data quality rules allows businesses to ensure that their data conforms to specific standards and requirements, which is crucial for accurate, reliable, and actionable data insights.

Data quality rules in Ataccama are closely integrated with the platform's data governance capabilities. This integration ensures that the enforcement of data quality is aligned with broader data governance frameworks and compliance requirements. Rules can be linked to specific data governance policies, providing traceability and accountability.

Ataccama ONE's \glsxtrshort{dqm} features are both robust and comprehensive, making it an exemplary foundation for any organization's data quality initiatives. The platform's extensive capabilities in defining, applying, monitoring, and resolving data quality issues position it as an ideal choice for those looking to enforce stringent data quality standards. With its rich suite of tools for data profiling, quality rule creation, and issue management, Ataccama ONE is well-equipped to handle modern enterprises' diverse and complex data landscapes. Given its robust feature set and integration potential, we will base our solution on Ataccama's proven frameworks to ensure that our data governance and quality management efforts are practical and efficient.

\section{The persona of Data engineer and their needs}

As we have established earlier, data engineer is the key persona in integrating data quality tools into data pipelines. It is good practice to tailor the application to its users and their requirements. It follows that any solution aimed at pipeline integration should consider the needs of data engineers.

\subsection{Used technologies}

Data engineers, like any others, are working professionals. Many of them are used to working with some particular tools and technologies. It is vital to take into account the tools and technologies that data engineers are familiar with when designing a solution for them.

Although among the many data engineers, there are differences in the tools and technologies they use, there are some common tools and technologies that are used by the majority of them in today's world. It is important to realize that we should not only consider an ideal data engineer, a skilled senior, but also consider that there are many junior data engineers. Also, it is important to consider the cognitive load of new tools and technologies on data engineers. Not only should the tools and technologies be easy to learn and use, but also they should be based on familiar concepts and technologies. This way, the data engineers can focus on their work and not on learning new tools and technologies. 

[Source needed] The most commonly used tools and technologies by data engineers are Python, Java, Scala, and SQL. These tools and technologies are used for the development of data pipelines and the integration of data quality tools into these pipelines. For any set of data engineers, the intersection of their knowledge bases will include Python more often than anything else.

\subsection{Ease of use}

In any software, a balance between ease of use and functionality complemented by the correctness of conceptual abstraction needs to be found. As the API surface of this solution is not going to be large, not a lot of decisions will have to be made on this front. However, it is important to keep in mind that the users of the solution are data engineers, some of whom work in a consultant background. Many of them are not used to advanced language features, and abstractions have limited experience with software engineering. For this reason, it is important to keep the solution simple and easy to use, avoiding any complex constructs and patterns.

\section{Data pipelines and requirements for the integration of data quality tools}

Data pipelines are a crucial part of any data engineering project. Data pipelines are used to move data from one place to another and to transform data from one format to another. 

Many of the use cases for integrating data quality tools into data pipelines include the requirement to integrate with existing data pipeline or solutions. The data quality tools should support integration into commonly used data pipelines. It also follows that forcing a new data pipeline or ETL solution is not a valid requirement. 

For example, in Ataccama, the application is intended to be connected to all the data sources and data targets using its custom connectors. To access the Ataccama engine and run any sort of evaluation of data quality rules, the data must be loaded into Ataccama ONE using a connection setup within the application, or the data needs to be sent into a service set up from within the application. Both approaches present a challenge for integrating Ataccama into existing data pipelines.

\subsection{Data security}

When designing data quality solutions for integration into existing data pipelines, especially those that involve interfacing with external applications or servers, security is a paramount concern. This is particularly critical when the data involved is sensitive, as is often the case in industries such as finance, healthcare, and government.

 Sending data over the internet to a third-party service can be a security risk. Data security is a major concern for data engineers, and it is important to take into account the security requirements of data engineers when designing a solution for them.

When a data quality integration in a pipeline needs to access the server - an application running somewhere else - the network of the server has to be accessible. This can be a security risk as every new network endpoint provides an additional entry point for attackers. When applications within a private network start communicating with external entities, these points of interaction need to be secured, adding complexity and potential for oversight.  If the networked application has vulnerabilities, such as insufficient authentication, flawed authorization practices, or software bugs, it could be exploited by attackers to gain unauthorized access. This could lead to data breaches, data loss, or malicious data manipulation. 


Additionally, data transmitted over networks can be intercepted, viewed, or altered by unauthorized parties if not adequately protected. This risk is particularly severe if data is transmitted over unsecured or improperly secured connections, such as those not using TLS/SSL protocols. 

\subsection{Ease of configuration}

The need to access a running instance of a \glsxtrshort{dqm} application in order to run data quality tooling comes with added complexities.

First, the application needs to be configured and running. This is fine for an environment where such an application is already in use. Yet, still, it is an added complexity as part of the process is running somewhere else, so it can be more difficult to debug, monitor, and maintain.

Second, the pipeline needs to access the application over a network. This means that the application needs to be exposed to the network, which can not only be a security risk but also provide further complexities in terms of network configuration. In case of the server being accessible only on a private network, the application needs to be exposed to the network, which can, in some cases, be even out of question and make the integration impossible, or it can be an obstacle on the way to successful integration.

\subsection{Pipelines in commonly used data platforms}

Modern data ecosystems are diverse, with organizations leveraging a variety of data storage solutions and computing environments to manage and analyze data. Here’s how Python integration plays a critical role across commonly used platforms:

\begin{itemize}
\item  Snowflake

Snowflake supports multiple programming languages, including Java and .NET, but Python remains a popular choice due to its extensive library support and community.

Python is well-supported in Snowflake through connectors like Snowflake Connector for Python, which allows executing SQL statements and performing data manipulations directly from Python scripts.


\item AWS Glue

    AWS Glue supports Python and Scala. Python, being one of the main languages supported by AWS Glue, benefits from seamless integration with other AWS services.
    
    Python scripts in AWS Glue can perform extract, transform, and load (ETL) tasks effectively, which can be developed and debugged directly in Python using Glue’s development endpoints.


    \item Azure Data Factories

    Azure Data Factory (ADF) supports custom activities in various languages, but Python’s use in Azure functions for custom processing activities is notable due to its simplicity and effectiveness.
   
    Python in ADF can be used to orchestrate complex data workflows, invoking Python-based processes as part of the data integration pipelines.


    \item Databricks

    Databricks offers a unified analytics platform that supports Python, Scala, SQL, and R. Python’s integration, particularly with PySpark for big data processing, makes it a primary choice for many developers.

    Python is extensively used in Databricks notebooks for data exploration, visualization, and machine learning, highlighting its versatility and ease of use.

\end{itemize}


Given the need to operate within commonly used compute platforms such as the above-mentioned, it is imperative to consider the compatibility of programming languages supported by these environments. Each platform offers support for various technologies; however, Python stands out due to its universal acceptance and extensive integration capabilities across these systems. Whether it is executing complex data manipulation tasks in Snowflake, orchestrating ETL processes in AWS Glue, running custom activities in Azure Data Factory, or performing data analysis and machine learning in Databricks, Python is consistently supported. 

Therefore, focusing on Python to implement data quality rules not only aligns with the operational capabilities of these platforms but also ensures that our solutions are versatile and adaptable across different technological ecosystems. This strategic choice maximizes the utility and reach of our \glsxtrshort{dqm} tools, making them accessible and functional within the predominant data processing frameworks employed by contemporary organizations.

\section{Similar Solutions}

This section compares Ataccama ONE to other \glsxtrshort{dqm} tools that are designed for programmatic access, highlighting the relative strengths and limitations of each solution in terms of features, technology stack, and integration capabilities.

\subsection{Soda Core}

Soda Core is a robust open-source tool tailored to integrate data quality checks directly into data pipelines. However, its feature set primarily focuses on:

\begin{itemize}
    \item Data Monitoring and Alerting
    
    Automatically detecting and alerting on anomalies in data as it flows through pipelines.

    \item Customizable Quality Checks

    While flexible, these are generally more basic compared to the depth provided by comprehensive DQM platforms.

    \item Python Integration
    
    Strong integration with Python-based data ecosystems, suitable for teams relying heavily on Python for data processing.
\end{itemize}

\subsection{Great Expectations}

Great Expectations offers a framework for setting up complex data validation and documentation, which is crucial for maintaining high data quality standards. Its key features include:

\begin{itemize}
    \item   Validation Framework
    
    Extensive support for defining expectations about data, which can be automatically validated against data batches.

    \item Data Docs
    
    Automatically generated documentation that helps keep teams aligned on data quality standards.

    \item Integration
    
    While it offers broad integration capabilities, it requires significant setup and maintenance compared to more out-of-the-box solutions.
\end{itemize}


\subsection{Comparison with Ataccama ONE}

Ataccama ONE offers a more extensive suite of features than either Soda Core or Great Expectations. This includes advanced functionalities like Master Data Management, Data Cataloging, and enriched Metadata Management, which are not typically found in the aforementioned open-source tools.

Ataccama integrates machine learning and AI-driven insights into its platform, providing a more sophisticated analysis and automation level than Soda Core and Great Expectations typically offer.

Despite its robust feature set, Ataccama’s main limitation lies in its less flexible integration with Python-based data pipelines, which is where Soda Core and Great Expectations excel due to their native Python support.

\section{Enhancing Ataccama's Integration Capabilities}

Given the comprehensive features of Ataccama ONE, the focus of this thesis will shift towards enhancing its integration capabilities with Python-based data pipelines to leverage its extensive data quality functionalities more effectively in programmatic environments.

While Ataccama offers a superior range of features, enhancing its programmatic accessibility would significantly increase its utility in modern data environments. This thesis will explore feasible solutions to bridge this gap, thereby extending the powerful capabilities of Ataccama ONE to a broader range of applications and use cases.
